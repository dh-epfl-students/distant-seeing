{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training note\n",
    "Necessary are all training, test and validation data\\\n",
    "Some code is sourced the following tutorial: https://github.com/qubvel/segmentation_models.pytorch/blob/master/examples/cars%20segmentation%20(camvid).ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dependencies \n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset as BaseDataset\n",
    "import torch\n",
    "import operator\n",
    "from PIL import Image, ImageDraw\n",
    "import albumentations as albu\n",
    "import segmentation_models_pytorch as smp\n",
    "import segmentation_models_pytorch.utils as smpu\n",
    "import random\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc\n",
    "#defined use gpu\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cd to working dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd '/scratch/ndillenb/notebooks/Hand-Segmentation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hands_file = '/scratch/ndillenb/notebooks/Hand-Segmentation/Hands_dataset/all_hands_2.json'\n",
    "hands_img = \"all_hands_2\"\n",
    "\n",
    "no_hands_file = '/scratch/ndillenb/notebooks/Hand-Segmentation/Hands_dataset/no_hands_2.json'\n",
    "no_hands_img = \"no_hands_2\"\n",
    "    \n",
    "x_hands_data = os.path.join('Hands_dataset',hands_file)\n",
    "y_hands_data = os.path.join('Hands_dataset',hands_img)\n",
    "x_no_hands_data = os.path.join('Hands_dataset',no_hands_file)\n",
    "y_no_hands_data = os.path.join('Hands_dataset',no_hands_img)\n",
    "\n",
    "all_dataset = 'all_dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for data visualization\n",
    "def visualize(**images):\n",
    "    \"\"\"PLot images in one row.\"\"\"\n",
    "    n = len(images)\n",
    "    plt.figure(figsize=(16, 5))\n",
    "    for i, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(' '.join(name.split('_')).title())\n",
    "        plt.imshow(image)\n",
    "    plt.show()\n",
    "# helper function for data permutation\n",
    "def apply_permutation(list1, list2):\n",
    "    indices = list(range(len(list1)))  # Generate a list of indices\n",
    "    random.shuffle(indices)  # Shuffle the indices randomly\n",
    "    \n",
    "    permuted_list1 = [list1[i] for i in indices]  # Apply permutation to list1\n",
    "    permuted_list2 = [list2[i] for i in indices]  # Apply permutation to list2\n",
    "    \n",
    "    return permuted_list1, permuted_list2\n",
    "# helper function for loss plotting\n",
    "def plot_loss(train_loss, valid_loss,num_epochs):\n",
    "    y1=[]\n",
    "    y2=[]\n",
    "    x = [i for i in range(1, num_epochs+1)]\n",
    "    fig = plt.figure()\n",
    "    for loss in train_loss:\n",
    "        y1.append(loss['jaccard_loss']) #need to be change to dice_loss if using dice_loss\n",
    "    for loss in valid_loss:\n",
    "        y2.append(loss['jaccard_loss']) #need to be change to dice_loss if using dice_loss\n",
    "    plt.plot(x,y1,color='r',label='training loss')\n",
    "    plt.plot(x,y2,color='g',label='validation loss')\n",
    "    plt.title('Training/Validation loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "def plot_metric(train_loss, valid_loss,num_epochs,metric_type):\n",
    "    fig = plt.figure()\n",
    "    y1=[]\n",
    "    y2=[]\n",
    "    x = [i for i in range(1, num_epochs+1)]\n",
    "    for loss in train_loss:\n",
    "        y1.append(loss[metric_type])\n",
    "    for loss in valid_loss:\n",
    "        y2.append(loss[metric_type])\n",
    "    plt.plot(x,y1,color='r',label='training')\n",
    "    plt.plot(x,y2,color='g',label='validation')\n",
    "    #plt.plot()\n",
    "    plt.title('Training/Validation '+str(metric_type))\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel(metric_type)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Dataset\n",
    "class Dataset(BaseDataset):\n",
    "    CLASSES = ['__BACKGROUND__', 'hand']\n",
    "    def __init__(self, \n",
    "        data_img,\n",
    "        data_file,  \n",
    "        images_dir,\n",
    "        classes=None, \n",
    "        augmentation=None, \n",
    "        preprocessing=None,):\n",
    "        \n",
    "        self.root = 'Hands_dataset'\n",
    "        self.folder = images_dir\n",
    "        \n",
    "        self.ids = data_img\n",
    "        self.images_fps = data_img\n",
    "        self.masks_fps = data_file\n",
    "        self.class_values = [self.CLASSES.index(cls.lower()) for cls in classes]\n",
    "        \n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "    def __getitem__(self, idx):\n",
    "        # load images and masks\n",
    "        img_path = os.path.join(self.root, self.folder, self.images_fps[idx])\n",
    "        img_out = cv2.imread(img_path)\n",
    "        img_out = cv2.cvtColor(img_out, cv2.COLOR_BGR2RGB)\n",
    "        mask_json = self.masks_fps[idx]\n",
    "        h,w,t = img_out.shape\n",
    "        all_polygon = []\n",
    "        im = Image.new(\"L\", (w,h), 0)\n",
    "        for shape in mask_json['annotation']:\n",
    "            polygon = tuple()\n",
    "            for point in shape['points']:\n",
    "                new_x = point['x']*w\n",
    "                new_y = point['y']*h\n",
    "                point = tuple([new_x,new_y])\n",
    "                polygon=polygon+point\n",
    "            all_polygon.append(polygon)\n",
    "            ImageDraw.Draw(im).polygon(polygon, outline=1,fill=1)\n",
    "        mask = np.array(im)\n",
    "        mask = mask[...,np.newaxis]\n",
    "        mask = np.stack(mask, axis=0).astype('int')\n",
    "        image = img_out\n",
    "        # apply augmentations\n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "        # apply preprocessing\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "        return image, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_augmentation():\n",
    "    train_transform = [\n",
    "        albu.HorizontalFlip(p=0.5),\n",
    "        #albu.ShiftScaleRotate(scale_limit=0.5, rotate_limit=0, shift_limit=0.1, p=1, border_mode=0),\n",
    "        #albu.Affine(scale=(1,0.5)), (USED BEFORE 03.05.2023)\n",
    "        albu.LongestMaxSize(max_size=160, interpolation=1, always_apply=True, p=1),\n",
    "        #albu.Resize(480,480,interpolation=1,always_apply=True,p=1)\n",
    "        #albu.PadIfNeeded(min_height=None,min_width=None,pad_height_divisor=32,pad_width_divisor=32,always_apply=True, border_mode=1)\n",
    "        albu.PadIfNeeded(min_height=160,min_width=160,always_apply=True, border_mode=0)\n",
    "        #albu.RandomCrop(height=320, width=320, always_apply=True),\n",
    "    ]\n",
    "    return albu.Compose(train_transform)\n",
    "\n",
    "\n",
    "def get_validation_augmentation():\n",
    "    \"\"\"Add paddings to make image shape divisible by 32\"\"\"\n",
    "    test_transform = [\n",
    "        #albu.geometric.resize.SmallestMaxSize(max_size=1600, interpolation=1, always_apply=True, p=1),\n",
    "        albu.LongestMaxSize(max_size=160, interpolation=1, always_apply=True, p=1),\n",
    "        #albu.Resize(480,480,interpolation=1,always_apply=True,p=1)\n",
    "        #albu.PadIfNeeded(min_height=None,min_width=None,pad_height_divisor=32,pad_width_divisor=32,always_apply=True, border_mode=1)\n",
    "        albu.PadIfNeeded(min_height=160,min_width=160,always_apply=True, border_mode=0)        #albu.RandomCrop(height=320, width=320, always_apply=True),\n",
    "        #albu.PadIfNeeded(384, 480)\n",
    "    ]\n",
    "    return albu.Compose(test_transform)\n",
    "\n",
    "\n",
    "def to_tensor(x, **kwargs):\n",
    "    return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "\n",
    "def get_preprocessing(preprocessing_fn):\n",
    "    \"\"\"Construct preprocessing transform\n",
    "    \n",
    "    Args:\n",
    "        preprocessing_fn (callbale): data normalization function \n",
    "            (can be specific for each pretrained neural network)\n",
    "    Return:\n",
    "        transform: albumentations.Compose\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    _transform = [\n",
    "        albu.Lambda(image=preprocessing_fn),\n",
    "        albu.Lambda(image=to_tensor, mask=to_tensor),\n",
    "    ]\n",
    "    return albu.Compose(_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENCODER = 'resnext101_32x8d'\n",
    "ENCODER_WEIGHTS = 'instagram'\n",
    "CLASSES = ['hand']\n",
    "ACTIVATION = 'sigmoid' # could be None for logits or 'softmax2d' for multiclass segmentation\n",
    "DEVICE = 'cuda'\n",
    "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create segmentation model with pretrained encoder\n",
    "model = smp.UnetPlusPlus( #Choice of architecture\n",
    "    encoder_name=ENCODER, \n",
    "    encoder_weights=ENCODER_WEIGHTS,\n",
    "    classes=len(CLASSES), \n",
    "    activation=ACTIVATION,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choice in datadistribution\n",
    "\n",
    "* 0.5 for half illustrations containing our feature, half not containing features\n",
    "* 0.33 for about 3 times mores illustrations that do not contains features as illustrations containing features\n",
    "* 0.1 for about 10 times mores illustrations that do not contains features as illustrations containing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SPLIT_TRAIN=0.5 #Here we pick as many illustrations containing features as not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hands_data_mask = json.load(open(x_hands_data))\n",
    "hands_data_mask = list(sorted(hands_data_mask['samples'],key=operator.itemgetter('imageUrl')))\n",
    "hands_data_img = list(sorted(os.listdir(y_hands_data)))\n",
    "hands_data_mask, hands_data_img = apply_permutation(hands_data_mask,hands_data_img)\n",
    "\n",
    "no_hands_data_mask = json.load(open(x_no_hands_data))\n",
    "no_hands_data_mask = list(sorted(no_hands_data_mask['samples'],key=operator.itemgetter('imageUrl')))\n",
    "no_hands_data_img = list(sorted(os.listdir(y_no_hands_data)))\n",
    "no_hands_data_mask, no_hands_data_img = apply_permutation(no_hands_data_mask,no_hands_data_img)\n",
    "\n",
    "if DATA_SPLIT_TRAIN == 0.5:\n",
    "    train_mask = hands_data_mask[0:173] + no_hands_data_mask[0:173]\n",
    "    train_img = hands_data_img[0:173] + no_hands_data_img[0:173]\n",
    "    train_mask, train_img = apply_permutation(train_mask,train_img)\n",
    "\n",
    "    valid_mask = hands_data_mask[173:194] + no_hands_data_mask[173:852]\n",
    "    valid_img = hands_data_img[173:194] + no_hands_data_img[173:852]\n",
    "    valid_mask, valid_img = apply_permutation(valid_mask,valid_img)\n",
    "\n",
    "    test_mask = hands_data_mask[194:215] + no_hands_data_mask[852:1531]\n",
    "    test_img = hands_data_img[194:215] + no_hands_data_img[852:1531]\n",
    "\n",
    "elif DATA_SPLIT_TRAIN == 0.33:\n",
    "    train_mask = hands_data_mask[0:173] + no_hands_data_mask[0:346]\n",
    "    train_img = hands_data_img[0:173] + no_hands_data_img[0:346]\n",
    "    train_mask, train_img = apply_permutation(train_mask,train_img)\n",
    "\n",
    "    valid_mask = hands_data_mask[173:194] + no_hands_data_mask[346:1025]\n",
    "    valid_img = hands_data_img[173:194] + no_hands_data_img[346:1025]\n",
    "    valid_mask, valid_img = apply_permutation(valid_mask,valid_img)\n",
    "\n",
    "    test_mask = hands_data_mask[194:215] + no_hands_data_mask[1025:1704]\n",
    "    test_img = hands_data_img[194:215] + no_hands_data_img[1025:1704]\n",
    "\n",
    "elif DATA_SPLIT_TRAIN == 0.1:\n",
    "    train_mask = hands_data_mask[0:173] + no_hands_data_mask[0:1730]\n",
    "    train_img = hands_data_img[0:173] + no_hands_data_img[0:1730]\n",
    "    train_mask, train_img = apply_permutation(train_mask,train_img)\n",
    "\n",
    "    valid_mask = hands_data_mask[173:194] + no_hands_data_mask[1730:2409]\n",
    "    valid_img = hands_data_img[173:194] + no_hands_data_img[1730:2409]\n",
    "    valid_mask, valid_img = apply_permutation(valid_mask,valid_img)\n",
    "\n",
    "    test_mask = hands_data_mask[194:215] + no_hands_data_mask[2409:3088]\n",
    "    test_img = hands_data_img[194:215] + no_hands_data_img[2409:3088]\n",
    "\n",
    "test_mask, test_img = apply_permutation(test_mask,test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataset = Dataset(\n",
    "    train_img, \n",
    "    train_mask, \n",
    "    all_dataset,\n",
    "    augmentation=get_training_augmentation(), \n",
    "    preprocessing=get_preprocessing(preprocessing_fn),\n",
    "    classes=CLASSES,\n",
    ")\n",
    "\n",
    "valid_dataset = Dataset(\n",
    "    valid_img, \n",
    "    valid_mask, \n",
    "    all_dataset,\n",
    "    augmentation=get_validation_augmentation(), \n",
    "    preprocessing=get_preprocessing(preprocessing_fn),\n",
    "    classes=CLASSES,\n",
    ")\n",
    "\n",
    "test_dataset = Dataset(\n",
    "    test_img, \n",
    "    test_mask, \n",
    "    all_dataset,\n",
    "    augmentation=get_validation_augmentation(), \n",
    "    preprocessing=get_preprocessing(preprocessing_fn),\n",
    "    classes=CLASSES,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=3, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize dataset sizes\n",
    "print('SPLIT = ' + 'TRAIN' +' | ' + 'VALID' + ' | ' +'TEST')\n",
    "print('SPLIT = ' + str(len(train_dataset)) +' | ' + str(len(valid_dataset)) + ' | ' +str(len(test_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the data to make sure all went smoothly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num = random.randint(0,len(train_dataset)-1)\n",
    "image, mask = train_dataset[num] # get some sample\n",
    "print(image.shape)\n",
    "print(mask.shape)\n",
    "visualize(\n",
    "    image=image.transpose(1, 2, 0).astype('float'), \n",
    "    hands_mask=mask.squeeze(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_tresh = 0.5 #Treshold used for separating positive from negative values\n",
    "\n",
    "#loss = smp.utils.losses.DiceLoss() #Choose this for a DiceLoss\n",
    "loss = smp.utils.losses.JaccardLoss() #Choose this for a JaccardLoss\n",
    "\n",
    "metrics = [ #Metrics we want to keep track of, some are usefull for debugging\n",
    "    smpu.metrics.IoU(threshold=current_tresh),\n",
    "    smpu.metrics.Fscore(threshold=current_tresh),\n",
    "    smpu.metrics.Accuracy(threshold=current_tresh),\n",
    "    smpu.metrics.Recall(threshold=current_tresh),\n",
    "    smpu.metrics.Precision(threshold=current_tresh)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choice of learning rate\n",
    "optimizer = torch.optim.Adam([dict(params=model.parameters(), lr=5.09E-05),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create epoch runners \n",
    "# it is a simple loop of iterating over dataloader's samples\n",
    "train_epoch = smp.utils.train.TrainEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    optimizer=optimizer,\n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "valid_epoch = smp.utils.train.ValidEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# train model for X epochs\n",
    "num_epochs = 6000 #Need to be changed to the max number of epochs\n",
    "max_score = 0\n",
    "train_logs = []\n",
    "valid_logs = []\n",
    "\n",
    "for i in range(len(train_logs), num_epochs):\n",
    "    print('\\nEpoch: {}'.format(i))\n",
    "    train_logs.append(train_epoch.run(train_loader))\n",
    "    valid_logs.append(valid_epoch.run(valid_loader))\n",
    "    if max_score < valid_logs[i]['iou_score']: #Save model locally if reached IoU highscore\n",
    "        max_score = valid_logs[i]['iou_score']\n",
    "        torch.save(model, './best_model.pth')\n",
    "        print('Model saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_epochs = len(train_logs)\n",
    "plot_loss(train_logs,valid_logs,len(train_logs))\n",
    "plot_metric(train_logs,valid_logs,num_epochs,'iou_score')\n",
    "plot_metric(train_logs,valid_logs,num_epochs,'fscore')\n",
    "plot_metric(train_logs,valid_logs,num_epochs,'accuracy')\n",
    "print('max IoU score : ' + str(max_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test best saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best saved checkpoint\n",
    "best_model = torch.load('./best_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# evaluate model on test set\n",
    "test_epoch = smp.utils.train.ValidEpoch(\n",
    "    model=best_model,\n",
    "    loss=loss,\n",
    "    metrics=metrics,\n",
    "    device=DEVICE,\n",
    ")\n",
    "test_epoch.run(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision-Recall curve and ROC Plot (Dataset-wise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing y_pred\n",
    "labels = []\n",
    "predictions = []\n",
    "predictions_arrays = []\n",
    "labels_array = []\n",
    "for i in range(len(test_dataset)):\n",
    "    label = test_dataset[i][1]\n",
    "    image = test_dataset[i][0]\n",
    "    x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n",
    "    prediction = best_model.predict(x_tensor)\n",
    "    prediction = (prediction.squeeze().cpu().numpy())\n",
    "    labels.append(label.squeeze().flatten())\n",
    "    predictions.append(prediction.squeeze().flatten())\n",
    "    predictions_arrays.append(prediction)\n",
    "    labels_array.append(label)\n",
    "    \n",
    "labels = np.concatenate(labels,axis=0)\n",
    "predictions = np.concatenate(predictions,axis=0)\n",
    "\n",
    "#comparing y_pred and y\n",
    "precision, recall, thresholds = precision_recall_curve(labels, predictions)\n",
    "fpr, tpr, tresholds = roc_curve(labels, predictions)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "#plotting P-R\n",
    "plt.figure()\n",
    "plt.plot(recall, precision)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.grid(True)\n",
    "num_thresholds = 5  # Number of thresholds to display\n",
    "lower_threshold = 0.1\n",
    "upper_threshold = 1\n",
    "step = max(1, len(thresholds) // num_thresholds)\n",
    "selected_thresholds = thresholds[(thresholds >= lower_threshold) & (thresholds <= upper_threshold)] #selecting between thresholds\n",
    "step = max(1, len(selected_thresholds) // num_thresholds)\n",
    "selected_thresholds = selected_thresholds[::step]\n",
    "for threshold in selected_thresholds:\n",
    "    index = np.where(thresholds == threshold)[0][0]\n",
    "    plt.annotate(f' {threshold:.2f}', (recall[index], precision[index])) #anotating thresholds\n",
    "plt.show()\n",
    "\n",
    "#plotting ROC\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line representing random guessing\n",
    "plt.xlabel('False Positive Rate (FPR)')\n",
    "plt.ylabel('True Positive Rate (TPR)')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imagewise and dataset-wise metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stacking y and y_pred\n",
    "labels_array_stacked_tensor = torch.from_numpy(np.stack(labels_array, axis=0)).to(torch.int32)\n",
    "predictions_arrays_stacked_tensor = torch.from_numpy(np.stack(np.expand_dims(predictions_arrays, axis=1), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 0.90 #treshold for separating positive from negative values\n",
    "\n",
    "tp, fp, fn, tn = smp.metrics.get_stats(predictions_arrays_stacked_tensor, labels_array_stacked_tensor, mode='binary', threshold=THRESHOLD)\n",
    "#List of metrics we wish to compute\n",
    "#All metrics are available at : https://smp.readthedocs.io/en/latest/metrics.html\n",
    "iou_score_micro = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\")\n",
    "iou_score_micro_iw = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"macro\")\n",
    "f1_score_micro = smp.metrics.f1_score(tp, fp, fn, tn, reduction=\"micro\")\n",
    "f1_score_micro_iw = smp.metrics.f1_score(tp, fp, fn, tn, reduction=\"micro-imagewise\")\n",
    "f2_score_micro = smp.metrics.fbeta_score(tp, fp, fn, tn, beta=2, reduction=\"micro\")\n",
    "accuracy_micro = smp.metrics.accuracy(tp, fp, fn, tn, reduction=\"micro\")\n",
    "accuracy_micro_iw = smp.metrics.accuracy(tp, fp, fn, tn, reduction=\"micro-imagewise\")\n",
    "recall_micro = smp.metrics.recall(tp, fp, fn, tn, reduction=\"micro\")\n",
    "recall_micro_iw = smp.metrics.recall(tp, fp, fn, tn, reduction=\"micro-imagewise\")\n",
    "precision_micro = smp.metrics.recall(tp, fp, fn, tn, reduction=\"micro\")\n",
    "precision_micro_iw = smp.metrics.recall(tp, fp, fn, tn, reduction=\"micro-imagewise\")\n",
    "fpr_micro = smp.metrics.false_positive_rate(tp, fp, fn, tn, reduction=\"micro\")\n",
    "fpr_micro_iw = smp.metrics.false_positive_rate(tp, fp, fn, tn, reduction=\"micro-imagewise\")\n",
    "\n",
    "stats = {'predict_array': predictions_arrays_stacked_tensor, 'labels_array': labels_array_stacked_tensor, 'test_tresh':THRESHOLD, 'tp':tp, 'fp':fp,'fn':fn,'tn':tn,\n",
    "        'iou_micro': iou_score_micro, 'iou_micro_iw': iou_score_micro_iw, 'f1_score_micro': f1_score_micro, 'f1_score_micro_iw':f1_score_micro_iw,\n",
    "        'f2_score_micro':f2_score_micro,'accuracy_micro':accuracy_micro,'accuracy_micro_iw':accuracy_micro_iw,'recall_micro':recall_micro,'recall_micro_iw':recall_micro_iw,\n",
    "        'precision_micro':precision_micro,'precision_micro_iw':precision_micro_iw,'fpr_micro':fpr_micro,'fpr_micro_iw':fpr_micro_iw}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the results and the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = '/trained_model_saved/'\n",
    "ARCHITECTURE = 'UnetPlusPlus' #Architecture used (too keep track)\n",
    "LOSS = 'JaccardLoss' #Loss used (too keep track)\n",
    "DATASET_NAME = 'half_split_test_val_distribution(50-3-3)-(80-10-10)' #How we named our datasplit (too keep track of)\n",
    "NEW_FOLDER = 'batch_16_size160_epoch1398_lr_5.09E-05' #Comment (should be unique if trained with same settings)\n",
    "path = os.path.join(parent_dir, ARCHITECTURE, ENCODER, ENCODER_WEIGHTS, LOSS, DATASET_NAME, NEW_FOLDER)\n",
    "os.makedirs(path, exist_ok=True) #create folders\n",
    "torch.save(model, os.path.join(path,'model.pth')) #save last models, if training needs to be resumed\n",
    "torch.save(best_model, os.path.join(path,'best_model.pth')) #save best model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-hand_seg_nm2]",
   "language": "python",
   "name": "conda-env-.conda-hand_seg_nm2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
